import torch
import numpy as np
from train.utilities import make_sequential_batches


def generating_latent_vector(model, test_dataset, features, s, latent_dim=128, index=False):
    """
    Generates the latent representation vectors of a given sequential model.

    Parameters
    ----------
    model : str
        Path to a PyTorch model
    test_dataset : str
        Path to the test set
    features :  list of str
        List of features
    s : int
        Sequence length of model
    latent_dim : int
        Latent dimension of model
    index :  bool
        If want to return id of market events
    Returns
    -------
    latent_vector: numpy array of size (len(test_dataset), latent_dim)
        Latent vectors generated by the model on every subsequence of length s of test_dataset
    fraudulent: bool numpy array of size (len(test_dataset), )
        Fraudulent order type contained in subsequence, if no fraud returns 0
    ids: numpy array of size (len(test_dataset), s)
        Ids of market events
    """
    # Creating batches
    batches = make_sequential_batches(test_dataset, s, features, batch_size=512)
    batch_size = batches.batch_size

    # Importing model
    if torch.cuda.is_available():
        model = torch.load(model).cuda()
    else:
        model = torch.load(model)
    model.eval()

    nb_batches = len(batches.dataset) // batch_size

    latent_vectors = np.zeros((nb_batches * batch_size, latent_dim))
    fraudulent = np.zeros(nb_batches * batch_size)
    ids = np.zeros((nb_batches * batch_size, s))

    # Computing latent vectors for every batch and determining if sequence had a fraud in it
    for count, (features, _) in enumerate(batches):
        if count < nb_batches:
            frauds = features[:, :, -1].numpy()
            if torch.cuda.is_available():
                features = features.cuda()

            if index:
                if model.__class__.__name__ == 'TransformerAutoencoder':
                    context = model.encoding(model.embedding(features[:, :, :-2]))
                elif model.__class__.__name__ == 'StackedLSTMAutoencoder':
                    context = model.encoding(features[:, :, :-2])[0]
                else:
                    context = model.encoding(features[:, :, :-2])
                ids[(count * batch_size): (count + 1) * batch_size, :] = features[:, :, -2].detach().cpu().numpy()
            else:
                if model.__class__.__name__ == 'TransformerAutoencoder':
                    context = model.encoding(model.embedding(features[:, :, :-1]))
                elif model.__class__.__name__ == 'StackedLSTMAutoencoder':
                    context = model.encoding(features[:, :, :-1])[0]
                else:
                    context = model.encoding(features[:, :, :-1])

            latent_vectors[(count * batch_size): (count + 1) * batch_size, :] = context.detach().cpu().numpy()
            fraudulent[(count * batch_size): (count + 1) * batch_size] = (np.count_nonzero(frauds, axis=1) >= 1) * \
                                                                         np.max(frauds, axis=1)

    return latent_vectors, fraudulent, ids


def generating_error_vector(model, test_dataset, features, s):
    """
    Generates the error vectors of a given sequential model.

    Parameters
    ----------
    model : str
        Path to a PyTorch model
    test_dataset : str
        Path to the test set
    features :  list of str
        List of features
    s : int
        Sequence length of model
    Returns
    -------
    errors: numpy array of size (len(test_dataset), )
        Errors generated by the model on every subsequence of length s of test_dataset
    fraudulent: bool numpy array of size (len(test_dataset), )
        Fraudulent order type contained in subsequence, if no fraud returns 0
    ids: numpy array of size (len(test_dataset), s)
        Ids of market events
    """
    # Creating batches
    batches = make_sequential_batches(test_dataset, s, features, batch_size=64)
    batch_size = batches.batch_size

    # Importing model
    if torch.cuda.is_available():
        model = torch.load(model).cuda()
    else:
        model = torch.load(model)
    model.eval()

    nb_batches = len(batches.dataset) // batch_size

    errors = np.zeros((nb_batches * batch_size))
    fraudulent = np.zeros(nb_batches * batch_size)
    ids = np.zeros((nb_batches * batch_size, s))

    # Computing latent vectors for every batch and determining if sequence had a fraud in it
    for count, (features, _) in enumerate(batches):
        if count < nb_batches:
            ids[(count * batch_size): (count + 1) * batch_size, :] = features[:, :, -2].numpy()
            if torch.cuda.is_available():
                features = features.cuda()
            if model.__class__.__name__ == 'TransformerAutoencoder':
                prediction = model(features[:, :, :-2], mode='generate')
            else:
                prediction = model(features[:, :, :-2])

            error = (features[:, :, :-2] - prediction).pow(2).sum(1).sum(1)
            errors[(count * batch_size): (count + 1) * batch_size] = error.detach().cpu().numpy()
            frauds = features[:, :, -1].detach().cpu().numpy()
            fraudulent[(count * batch_size): (count + 1) * batch_size] = (np.count_nonzero(frauds, axis=1) >= 1) * \
                                                                         np.max(frauds, axis=1)

    return errors, fraudulent, ids


def generating_sequence_vector(test_dataset, features, s, index=False):
    """
    Generates the sequences of the test dataset.

    Parameters
    ----------
    test_dataset : str
        Path to the test set
    features :  list of str
        List of features
    s : int
        Sequence length of model
    index :  bool
        If want to return id of market events
    Returns
    -------
    sequence_vector: numpy array of size (len(test_dataset), len(features) - 1) * s or len(features) - 2) * s)
        All sequences of test_dataset
    fraudulent: bool numpy array of size (len(test_dataset), )
        Fraudulent order type contained in subsequence, if no fraud returns 0
    ids: numpy array of size (len(test_dataset), s)
        Ids of market events
    """
    # Creating batches
    batches = make_sequential_batches(test_dataset, s, features)
    batch_size = batches.batch_size
    nb_batches = len(batches.dataset) // batch_size

    if index:
        sequence_vectors = np.zeros((nb_batches * batch_size, (len(features) - 2) * s))
    else:
        sequence_vectors = np.zeros((nb_batches * batch_size, (len(features) - 1) * s))
    fraudulent = np.zeros(nb_batches * batch_size)
    ids = np.zeros((nb_batches * batch_size, s))

    # Getting sequence for every batch and determining if sequence had a fraud in it
    for count, (features, _) in enumerate(batches):
        if count < nb_batches:
            if index:
                input_features = features[:, :, :-2]
                ids[(count * batch_size): (count + 1) * batch_size, :] = features[:, :, -2].numpy()
            else:
                input_features = features[:, :, :-1]

            sequence_vectors[(count * batch_size): (count + 1) * batch_size, :] = input_features.reshape((batch_size,
                                                                                                          -1)).numpy()
            frauds = features[:, :, -1].numpy()
            fraudulent[(count * batch_size): (count + 1) * batch_size] = (np.count_nonzero(frauds, axis=1) >= 1) * \
                                                                         np.max(frauds, axis=1)
    return sequence_vectors, fraudulent, ids



